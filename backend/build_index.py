# build_index.py
import os
import argparse
import json
import duckdb
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io

from haystack import Document
from haystack.components.preprocessors import DocumentSplitter
from haystack.components.embedders import SentenceTransformersDocumentEmbedder


# ------------------------------
# 1. ê²½ë¡œ ì„¤ì •
# ------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "hibot_store.db")
DATA_PATH = os.path.join(BASE_DIR, "../hibot-chat-docs-pdf")

EMBEDDING_MODEL = "jhgan/ko-sbert-nli"


# ------------------------------
# 2. DuckDB Document Store
# ------------------------------
class DuckDBDocumentStore:
    def __init__(self, db_path):
        self.conn = duckdb.connect(db_path)
        self._setup_tables()

    def _setup_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                meta TEXT,
                embedding DOUBLE[]
            )
        """)
        self.conn.commit()

    def write_documents(self, documents):
        for doc in documents:
            meta_json = json.dumps(doc.meta) if doc.meta else "{}"

            # embedding â†’ Python listë¡œ ì²˜ë¦¬
            if doc.embedding is not None:
                if hasattr(doc.embedding, "tolist"):
                    embed_list = doc.embedding.tolist()
                else:
                    embed_list = list(doc.embedding)
            else:
                embed_list = None

            self.conn.execute("""
                INSERT OR REPLACE INTO documents (id, content, meta, embedding)
                VALUES (?, ?, ?, ?)
            """, (str(doc.id), doc.content, meta_json, embed_list))

        self.conn.commit()
        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    def filter_documents(self, filters=None):
        query = "SELECT id, content, meta, embedding FROM documents"
        result = self.conn.execute(query).fetchall()

        documents = []
        for row in result:
            doc_id, content, meta_str, embedding = row
            meta = json.loads(meta_str) if meta_str else {}

            documents.append(Document(
                id=doc_id,
                content=content,
                meta=meta,
                embedding=embedding
            ))
        return documents

    def delete_all_documents(self):
        self.conn.execute("DELETE FROM documents")
        self.conn.commit()
        print("ğŸ—‘ï¸ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    def count_documents(self):
        return self.conn.execute("SELECT COUNT(*) FROM documents").fetchone()[0]


# ------------------------------
# 3. OCR ì§€ì› PDF â†’ Text ë³€í™˜ê¸°
# ------------------------------
def extract_text_with_ocr(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""

    for page in doc:
        # (1) ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text += page.get_text("text") + "\n"

        # (2) ì´ë¯¸ì§€ OCR ì²˜ë¦¬
        for img in page.get_images(full=True):
            xref = img[0]
            base = doc.extract_image(xref)
            image_bytes = base["image"]

            image = Image.open(io.BytesIO(image_bytes))
            ocr_text = pytesseract.image_to_string(image, lang="kor+eng")
            full_text += ocr_text + "\n"

    return full_text


# ------------------------------
# 4. PDF â†’ Haystack Document ë³€í™˜
# ------------------------------
def convert_pdf_to_documents(pdf_path, file_name):
    text = extract_text_with_ocr(pdf_path)
    return [
        Document(
            content=text,
            meta={"file_name": file_name}
        )
    ]


# ------------------------------
# 5. ë©”ì¸ ìƒ‰ì¸ ë¡œì§
# ------------------------------
def main(force_rebuild=False):
    print("DATA_PATH:", DATA_PATH)
    print("ë¬¸ì„œ ìƒ‰ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1) DB ì´ˆê¸°í™”
    store = DuckDBDocumentStore(DB_PATH)

    # --force ì˜µì…˜ì´ë©´ ì „ì²´ ì‚­ì œ
    if force_rebuild:
        print("âš ï¸ --force ì˜µì…˜ ê°ì§€ â†’ ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì¤‘â€¦")
        store.delete_all_documents()

    # DBì— ì €ì¥ëœ íŒŒì¼ ëª©ë¡
    existing_docs = store.filter_documents()
    indexed_files = {d.meta.get("file_name") for d in existing_docs if d.meta.get("file_name")}

    print(f"âœ… DBì— ê¸°ë¡ëœ PDF íŒŒì¼ ìˆ˜: {len(indexed_files)}")

    # ì‹¤ì œ í´ë”ì— ì¡´ì¬í•˜ëŠ” PDF ëª©ë¡
    if not os.path.exists(DATA_PATH):
        print("âŒ PDF í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤:", DATA_PATH)
        return

    pdf_files = {f for f in os.listdir(DATA_PATH) if f.endswith(".pdf")}
    new_files = pdf_files - indexed_files

    if not new_files:
        print("âœ… ìƒˆë¡œ ìƒ‰ì¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš¨ ìƒˆ PDF ë°œê²¬ â†’ {len(new_files)}ê°œ ìƒ‰ì¸ ì‹œì‘: {list(new_files)}")

    # ë¬¸ì„œ ë¶„í• ê¸°
    splitter = DocumentSplitter(
        split_by="word",
        split_length=700,
        split_overlap=150
    )

    splitter.warm_up()

    # ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸
    embedder = SentenceTransformersDocumentEmbedder(model=EMBEDDING_MODEL)
    embedder.warm_up()

    # âœ… ìƒˆ íŒŒì¼ë“¤ ìƒ‰ì¸
    for file_name in new_files:
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")

        pdf_path = os.path.join(DATA_PATH, file_name)

        # (1) OCR í¬í•¨ PDF â†’ Document ë³€í™˜
        docs = convert_pdf_to_documents(pdf_path, file_name)

        # (2) ë¬¸ì¥ ë‹¨ìœ„ chunking
        split_docs = splitter.run(docs)["documents"]

        # (3) ì„ë² ë”©
        embedded_docs = embedder.run(split_docs)["documents"]

        # (4) DB ì €ì¥
        store.write_documents(embedded_docs)

    print("âœ… ëª¨ë“  ìƒˆ PDF ìƒ‰ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜:", store.count_documents())


# ------------------------------
# 6. ì‹¤í–‰ë¶€
# ------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œ í›„ ì „ì²´ ì¬ìƒ‰ì¸")
    args = parser.parse_args()

    main(force_rebuild=args.force)
