# build_index.py
import os
import argparse
import json
import duckdb
# import fitz  # PyMuPDF
# import pytesseract
# from PIL import Image
# import io

from haystack import Document
from haystack.components.preprocessors import DocumentSplitter
from haystack.components.embedders import SentenceTransformersDocumentEmbedder

# hwpë³€í™˜ 
from extract_text.hwpToHwpx import convert_hwp_to_hwpx
from extract_text.extract_hwpx_text import extract_text_from_hwpx

# ------------------------------
# 1. ê²½ë¡œ ì„¤ì •
# ------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "hibot_store.db")
DATA_PATH = os.path.join(BASE_DIR, "../hibot-chat-docs-hwp")

EMBEDDING_MODEL = "jhgan/ko-sbert-nli"


# ------------------------------
# 2. DuckDB Document Store
# ------------------------------
class DuckDBDocumentStore:
    def __init__(self, db_path):
        self.conn = duckdb.connect(db_path)
        self._setup_tables()

    def _setup_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                meta TEXT,
                embedding DOUBLE[]
            )
        """)
        self.conn.commit()

    def write_documents(self, documents):
        for doc in documents:
            meta_json = json.dumps(doc.meta) if doc.meta else "{}"

            # embedding â†’ Python listë¡œ ì²˜ë¦¬
            if doc.embedding is not None:
                if hasattr(doc.embedding, "tolist"):
                    embed_list = doc.embedding.tolist()
                else:
                    embed_list = list(doc.embedding)
            else:
                embed_list = None

            self.conn.execute("""
                INSERT OR REPLACE INTO documents (id, content, meta, embedding)
                VALUES (?, ?, ?, ?)
            """, (str(doc.id), doc.content, meta_json, embed_list))

        self.conn.commit()
        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    def filter_documents(self, filters=None):
        query = "SELECT id, content, meta, embedding FROM documents"
        result = self.conn.execute(query).fetchall()

        documents = []
        for row in result:
            doc_id, content, meta_str, embedding = row
            meta = json.loads(meta_str) if meta_str else {}

            documents.append(Document(
                id=doc_id,
                content=content,
                meta=meta,
                embedding=embedding
            ))
        return documents

    def delete_all_documents(self):
        self.conn.execute("DELETE FROM documents")
        self.conn.commit()
        print("ğŸ—‘ï¸ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    def count_documents(self):
        return self.conn.execute("SELECT COUNT(*) FROM documents").fetchone()[0]


# ------------------------------
# 5. ë©”ì¸ ìƒ‰ì¸ ë¡œì§
# ------------------------------
def main(force_rebuild=False):
    print("DATA_PATH:", DATA_PATH)
    print("ë¬¸ì„œ ìƒ‰ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1) DB ì´ˆê¸°í™”
    store = DuckDBDocumentStore(DB_PATH)

    # --force ì˜µì…˜ì´ë©´ ì „ì²´ ì‚­ì œ
    if force_rebuild:
        print("âš ï¸ --force ì˜µì…˜ ê°ì§€ â†’ ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì¤‘â€¦")
        store.delete_all_documents()

    # DBì— ì €ì¥ëœ íŒŒì¼ ëª©ë¡
    existing_docs = store.filter_documents()
    indexed_files = {d.meta.get("file_name") for d in existing_docs if d.meta.get("file_name")}

    print(f"âœ… DBì— ê¸°ë¡ëœ PDF íŒŒì¼ ìˆ˜: {len(indexed_files)}")

    # ì‹¤ì œ í´ë”ì— ì¡´ì¬í•˜ëŠ” PDF ëª©ë¡
    if not os.path.exists(DATA_PATH):
        print("âŒ PDF í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤:", DATA_PATH)
        return

    
    # pdf_files = {f for f in os.listdir(DATA_PATH) if f.endswith(".pdf")}
    # new_files = pdf_files - indexed_files
    hwp_files = {f for f in os.listdir(DATA_PATH) if f.endswith(".hwp")}
    new_files = hwp_files - indexed_files

    if not new_files:
        print("âœ… ìƒˆë¡œ ìƒ‰ì¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš¨ ìƒˆ PDF ë°œê²¬ â†’ {len(new_files)}ê°œ ìƒ‰ì¸ ì‹œì‘: {list(new_files)}")

    # ë¬¸ì„œ ë¶„í• ê¸°
    splitter = DocumentSplitter(split_by="sentence", split_length=5)
    splitter.warm_up()

    # ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸
    embedder = SentenceTransformersDocumentEmbedder(model=EMBEDDING_MODEL)
    embedder.warm_up()

    # âœ… ìƒˆ íŒŒì¼ë“¤ ìƒ‰ì¸
    for file_name in new_files:
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")

        hwp_path = os.path.join(DATA_PATH, file_name)

        # (1) HWP â†’ HWPX ë³€í™˜
        hwpx_path = convert_hwp_to_hwpx(hwp_path)

        # (2) HWPX â†’ TEXT ì¶”ì¶œ
        text = extract_text_from_hwpx(hwpx_path)

        docs = [Document(content=text, meta={"file_name": file_name})]

        # (2) ë¬¸ì¥ ë‹¨ìœ„ chunking
        split_docs = splitter.run(docs)["documents"]

        # (3) ì„ë² ë”©
        embedded_docs = embedder.run(split_docs)["documents"]

        # (4) DB ì €ì¥
        store.write_documents(embedded_docs)

    print("âœ… ëª¨ë“  ìƒˆ PDF ìƒ‰ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜:", store.count_documents())


# ------------------------------
# 6. ì‹¤í–‰ë¶€
# ------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œ í›„ ì „ì²´ ì¬ìƒ‰ì¸")
    args = parser.parse_args()

    main(force_rebuild=args.force)
